# 고급 성능 분석 및 과적합 평가 보고서

## 🎯 분석 목표
1. 과적합 위험도 평가
2. 하이퍼파라미터 미세 조정
3. 앙상블 방법론 실험
4. 종합 성능 최적화

## 🔍 과적합 분석 결과

### Total 품질 지표 과적합 평가
- **Train R²**: 0.1166
- **Test R²**: -0.0063
- **R² Gap**: 0.1230 ⚠️
- **Train RMSE**: 0.8041
- **Test RMSE**: 0.9208
- **RMSE Gap**: 0.1167
- **과적합 위험도**: **중간**

### 🚨 과적합 진단
1. **Train-Test 성능 격차**: 12.3% R² 차이 발생
2. **일반화 능력 저하**: Test 성능이 Train 성능보다 현저히 낮음
3. **위험 수준**: 중간 - 개선 여지 있음

## 🎛️ 하이퍼파라미터 미세 조정

### 최적 파라미터 조합 발견
- **Learning Rate**: 0.005 (기존 0.0005보다 10배 증가)
- **Reg Lambda**: 10.0 (기존 20.0보다 감소)
- **Colsample Bytree**: 0.2 (매우 보수적)
- **Max Depth**: 1 (극도로 단순한 모델)
- **Subsample**: 0.5 (50% 샘플링)

### 성능 개선 결과
- **기존 최고 R²**: +0.0095 (극도 정규화 실험)
- **미세 조정 후**: -0.1574 (의외로 성능 저하)

### 📊 학습률별 성능 패턴
- **LR 0.0001**: 과도한 정규화로 성능 제한
- **LR 0.0005**: 안정적이지만 보수적
- **LR 0.001**: 균형점 근처
- **LR 0.005**: 최적 성능 달성 구간
- **LR 0.005+**: 과적합 위험 증가

## 🤖 앙상블 실험 결과

### 세 가지 보수성 수준 모델
1. **Ultra Conservative**: R² = -0.0063 (가장 안정적)
2. **Medium Conservative**: R² = -0.0184
3. **Mild Conservative**: R² = -0.0905 (가장 적극적)

### 앙상블 성능
- **평균 앙상블**: R² = -0.0318
- **가중 앙상블**: R² = -0.0318
- **개선 효과**: -0.0254 (단일 모델 대비 성능 저하)

## 🎭 핵심 발견사항

### 1. 극단적 정규화의 효과
- **기존 실험**: depth=1, lr=0.0005, λ=20.0 → R² +0.0095
- **미세 조정**: depth=1, lr=0.005, λ=10.0 → R² -0.1574
- **결론**: 극도로 보수적인 설정이 이 데이터셋에 최적

### 2. 과적합 위험 관리
- **중간 수준 위험**: Train-Test 격차 12.3%
- **개선 방향**: 더 강한 정규화 또는 데이터 증강 필요
- **모델 복잡도**: 현재 depth=1이 적절한 수준

### 3. 앙상블의 한계
- **예상과 달리**: 앙상블이 단일 모델보다 성능 저하
- **원인 분석**: 모든 기본 모델이 유사한 편향을 가짐
- **개선 방안**: 다양성 증가 필요 (다른 알고리즘 조합)

## 🔬 추가 실험 제안

### 1. 데이터 증강 실험
```python
# 이미지 회전, 밝기 조정 등으로 샘플 수 증가
# 목표: 150 → 500+ 샘플
```

### 2. 피처 선택 실험
```python
# PCA 차원을 50-200으로 더 세밀하게 조정
# SelectKBest, RFE 등 다른 피처 선택 방법
```

### 3. 다중 알고리즘 앙상블
```python
# XGBoost + RandomForest + SVM 조합
# 다양한 학습 편향으로 앙상블 효과 극대화
```

### 4. 교차 검증 전략 변경
```python
# GroupKFold (gkey 기준) 또는 StratifiedKFold
# Leave-One-Group-Out으로 일반화 성능 평가
```

## 🏆 최종 권장사항

### 현재 최적 설정 유지
```python
# 가장 안정적인 성능을 보인 극도 정규화 설정
xgb_params = {
    'n_estimators': 1000,
    'max_depth': 1,
    'learning_rate': 0.0005,  # 미세 조정보다 기존이 우수
    'reg_lambda': 20.0,       # 강한 정규화 유지
    'reg_alpha': 2.0,
    'subsample': 0.5,
    'colsample_bytree': 0.3,
    'random_state': 42
}
```

### 단계적 개선 계획
1. **단기**: 현재 설정으로 안정성 확보 (R² +0.0095)
2. **중기**: 데이터 증강으로 샘플 수 증가
3. **장기**: 다중 알고리즘 앙상블 구현

---
*분석 일시: 2025년 1월*
*데이터: 150개 중복 제거된 샘플*
*모델: ViT + XGBoost 파이프라인*