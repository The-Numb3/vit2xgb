#!/usr/bin/env python3
"""
ê³¼ì í•© ìœ„í—˜ë„ í‰ê°€ ë° ì¶”ê°€ ì„±ëŠ¥ í–¥ìƒ ì‹¤í—˜
"""

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, validation_curve
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

def load_data():
    """ë°ì´í„° ë¡œë”©"""
    features = np.load("vit_out_demo/features.npy")
    meta_df = pd.read_csv("vit_out_demo/meta.csv")
    return features, meta_df

def overfitting_analysis(X, y, target_name):
    """ê³¼ì í•© ë¶„ì„"""
    print(f"\nğŸ” ê³¼ì í•© ë¶„ì„: {target_name}")
    print("="*50)
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y.astype(int)
    )
    
    # ìµœì  ì„¤ì •ìœ¼ë¡œ ëª¨ë¸ í•™ìŠµ
    model = xgb.XGBRegressor(
        n_estimators=2000,
        max_depth=1,
        learning_rate=0.0005,
        subsample=0.5,
        colsample_bytree=0.3,
        reg_alpha=2.0,
        reg_lambda=20.0,
        random_state=42,
        verbosity=0
    )
    
    # í•™ìŠµ ê³¡ì„  ëª¨ë‹ˆí„°ë§
    eval_set = [(X_train, y_train), (X_test, y_test)]
    
    model.fit(X_train, y_train)
    
    # ì˜ˆì¸¡
    train_pred = model.predict(X_train)
    test_pred = model.predict(X_test)
    
    # ì„±ëŠ¥ ì§€í‘œ
    train_r2 = r2_score(y_train, train_pred)
    test_r2 = r2_score(y_test, test_pred)
    train_rmse = np.sqrt(mean_squared_error(y_train, train_pred))
    test_rmse = np.sqrt(mean_squared_error(y_test, test_pred))
    
    # ê³¼ì í•© ì§€í‘œ
    r2_gap = train_r2 - test_r2
    rmse_gap = test_rmse - train_rmse
    
    print(f"Train RÂ²: {train_r2:.4f}")
    print(f"Test RÂ²:  {test_r2:.4f}")
    print(f"RÂ² Gap:   {r2_gap:.4f} ({'âš ï¸ ê³¼ì í•©' if r2_gap > 0.1 else 'âœ… ì •ìƒ'})")
    print(f"Train RMSE: {train_rmse:.4f}")
    print(f"Test RMSE:  {test_rmse:.4f}")  
    print(f"RMSE Gap:   {rmse_gap:.4f} ({'âš ï¸ ê³¼ì í•©' if rmse_gap > 0.1 else 'âœ… ì •ìƒ'})")
    
    # ê³¼ì í•© ìœ„í—˜ë„ í‰ê°€
    risk_level = "ë‚®ìŒ"
    if r2_gap > 0.2 or rmse_gap > 0.15:
        risk_level = "ë†’ìŒ"
    elif r2_gap > 0.1 or rmse_gap > 0.1:
        risk_level = "ì¤‘ê°„"
    
    print(f"ê³¼ì í•© ìœ„í—˜ë„: {risk_level}")
    
    return {
        'train_r2': train_r2,
        'test_r2': test_r2, 
        'r2_gap': r2_gap,
        'train_rmse': train_rmse,
        'test_rmse': test_rmse,
        'rmse_gap': rmse_gap,
        'risk_level': risk_level,
        'model': model
    }

def hyperparameter_fine_tuning(X, y, target_name):
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì •"""
    print(f"\nğŸ›ï¸ í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì •: {target_name}")
    print("="*50)
    
    # í…ŒìŠ¤íŠ¸í•  íŒŒë¼ë¯¸í„° ë²”ìœ„
    param_ranges = {
        'learning_rate': [0.0001, 0.0005, 0.001, 0.005],
        'reg_lambda': [5.0, 10.0, 20.0, 50.0],
        'colsample_bytree': [0.2, 0.3, 0.4, 0.5],
    }
    
    best_score = -np.inf
    best_params = None
    results = []
    
    # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
    base_params = {
        'n_estimators': 1000,
        'max_depth': 1,
        'subsample': 0.5,
        'reg_alpha': 2.0,
        'random_state': 42,
        'verbosity': 0
    }
    
    for lr in param_ranges['learning_rate']:
        for reg_lambda in param_ranges['reg_lambda']:
            for colsample in param_ranges['colsample_bytree']:
                
                params = base_params.copy()
                params.update({
                    'learning_rate': lr,
                    'reg_lambda': reg_lambda,
                    'colsample_bytree': colsample
                })
                
                # 3-fold CVë¡œ ë¹ ë¥¸ í‰ê°€
                from sklearn.model_selection import cross_val_score
                model = xgb.XGBRegressor(**params)
                
                try:
                    scores = cross_val_score(model, X, y, cv=3, scoring='r2')
                    mean_score = scores.mean()
                    std_score = scores.std()
                    
                    results.append({
                        'lr': lr,
                        'reg_lambda': reg_lambda, 
                        'colsample': colsample,
                        'r2_mean': mean_score,
                        'r2_std': std_score
                    })
                    
                    if mean_score > best_score:
                        best_score = mean_score
                        best_params = params.copy()
                        
                    print(f"LR:{lr:7.4f} | Î»:{reg_lambda:4.1f} | Col:{colsample:.1f} | RÂ²:{mean_score:+.4f}Â±{std_score:.4f}")
                    
                except Exception as e:
                    print(f"LR:{lr:7.4f} | Î»:{reg_lambda:4.1f} | Col:{colsample:.1f} | Error: {e}")
    
    print(f"\nğŸ† ìµœì  íŒŒë¼ë¯¸í„° (RÂ² = {best_score:.4f}):")
    for k, v in best_params.items():
        print(f"  {k}: {v}")
    
    return best_params, pd.DataFrame(results)

def ensemble_experiment(X, y, target_name):
    """ì•™ìƒë¸” ì‹¤í—˜"""
    print(f"\nğŸ¤– ì•™ìƒë¸” ì‹¤í—˜: {target_name}")
    print("="*50)
    
    # ë°ì´í„° ë¶„í• 
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.3, random_state=42, stratify=y.astype(int)
    )
    
    # ë‹¤ì–‘í•œ ì„¤ì •ì˜ ëª¨ë¸ë“¤
    models = {
        'Ultra_Conservative': xgb.XGBRegressor(
            n_estimators=2000, max_depth=1, learning_rate=0.0005,
            subsample=0.5, colsample_bytree=0.3, reg_alpha=2.0, reg_lambda=20.0,
            random_state=42, verbosity=0
        ),
        'Medium_Conservative': xgb.XGBRegressor(
            n_estimators=1000, max_depth=2, learning_rate=0.001,
            subsample=0.6, colsample_bytree=0.4, reg_alpha=1.0, reg_lambda=10.0,
            random_state=43, verbosity=0
        ),
        'Mild_Conservative': xgb.XGBRegressor(
            n_estimators=500, max_depth=2, learning_rate=0.005,
            subsample=0.7, colsample_bytree=0.5, reg_alpha=0.5, reg_lambda=5.0,
            random_state=44, verbosity=0
        )
    }
    
    # ê°œë³„ ëª¨ë¸ ì„±ëŠ¥
    predictions = {}
    individual_scores = {}
    
    for name, model in models.items():
        model.fit(X_train, y_train)
        pred = model.predict(X_test)
        predictions[name] = pred
        individual_scores[name] = r2_score(y_test, pred)
        print(f"{name}: RÂ² = {individual_scores[name]:.4f}")
    
    # ì•™ìƒë¸” (í‰ê· )
    ensemble_pred = np.mean(list(predictions.values()), axis=0)
    ensemble_r2 = r2_score(y_test, ensemble_pred)
    
    print(f"\nğŸ† ì•™ìƒë¸” (í‰ê· ): RÂ² = {ensemble_r2:.4f}")
    
    # ê°€ì¤‘ ì•™ìƒë¸” (ì„±ëŠ¥ ê¸°ë°˜ ê°€ì¤‘ì¹˜)
    weights = np.array(list(individual_scores.values()))
    weights = np.maximum(weights, 0)  # ìŒìˆ˜ ì ìˆ˜ëŠ” 0ìœ¼ë¡œ
    weights = weights / weights.sum() if weights.sum() > 0 else np.ones_like(weights) / len(weights)
    
    weighted_pred = np.average(list(predictions.values()), axis=0, weights=weights)
    weighted_r2 = r2_score(y_test, weighted_pred)
    
    print(f"ğŸ¯ ê°€ì¤‘ ì•™ìƒë¸”: RÂ² = {weighted_r2:.4f}")
    print(f"   ê°€ì¤‘ì¹˜: {dict(zip(models.keys(), weights.round(3)))}")
    
    return {
        'individual': individual_scores,
        'ensemble': ensemble_r2,
        'weighted_ensemble': weighted_r2,
        'best_individual': max(individual_scores.values()),
        'ensemble_improvement': ensemble_r2 - max(individual_scores.values())
    }

def main():
    print("ğŸ”¬ ê³ ê¸‰ ì„±ëŠ¥ í–¥ìƒ ë° ê³¼ì í•© ë¶„ì„")
    print("="*60)
    
    # ë°ì´í„° ë¡œë“œ
    features, meta_df = load_data()
    
    # Total íƒ€ê²Ÿ ë¶„ì„
    total_mask = pd.notna(meta_df['Total'])
    X_total = features[total_mask]
    y_total = meta_df.loc[total_mask, 'Total'].values
    
    print(f"ë¶„ì„ ë°ì´í„°: {len(X_total)}ê°œ ìƒ˜í”Œ, {X_total.shape[1]}ì°¨ì›")
    
    # 1. ê³¼ì í•© ë¶„ì„
    overfitting_results = overfitting_analysis(X_total, y_total, "Total")
    
    # 2. í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¯¸ì„¸ ì¡°ì •
    best_params, tuning_results = hyperparameter_fine_tuning(X_total, y_total, "Total")
    
    # 3. ì•™ìƒë¸” ì‹¤í—˜  
    ensemble_results = ensemble_experiment(X_total, y_total, "Total")
    
    # ê²°ê³¼ ìš”ì•½
    print(f"\nğŸ“Š ì¢…í•© ê²°ê³¼ ìš”ì•½")
    print("="*60)
    print(f"ê³¼ì í•© ìœ„í—˜ë„: {overfitting_results['risk_level']}")
    print(f"Train-Test RÂ² Gap: {overfitting_results['r2_gap']:.4f}")
    print(f"ìµœì  ë‹¨ì¼ ëª¨ë¸ RÂ²: {ensemble_results['best_individual']:.4f}")
    print(f"ì•™ìƒë¸” RÂ²: {ensemble_results['ensemble']:.4f}")
    print(f"ê°€ì¤‘ ì•™ìƒë¸” RÂ²: {ensemble_results['weighted_ensemble']:.4f}")
    print(f"ì•™ìƒë¸” ê°œì„ : {ensemble_results['ensemble_improvement']:+.4f}")
    
    # ê²°ê³¼ ì €ì¥
    tuning_results.to_csv("hyperparameter_tuning_results.csv", index=False)
    
    return {
        'overfitting': overfitting_results,
        'tuning': tuning_results,
        'ensemble': ensemble_results
    }

if __name__ == "__main__":
    main()